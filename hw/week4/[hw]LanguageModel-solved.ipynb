{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vr4atkrm_QF-"
   },
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "random.seed(random_seed)\n",
    "\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "# Включите куду\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "os.environ['PYTHONHASHSEED'] = str(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9549,
     "status": "ok",
     "timestamp": 1554403042447,
     "user": {
      "displayName": "Taras Khakhulin",
      "photoUrl": "https://lh6.googleusercontent.com/-FZErFAXozr4/AAAAAAAAAAI/AAAAAAAAQVc/3T-aGco2864/s64/photo.jpg",
      "userId": "10057679887987544812"
     },
     "user_tz": -180
    },
    "id": "f_V0UqK-6UJy",
    "outputId": "3590727c-570a-4c26-8749-8cea045352f6"
   },
   "outputs": [],
   "source": [
    "#! wget https://raw.githubusercontent.com/DLSchool/dlschool_old/master/materials/homeworks/hw09/dostoevsky.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zVfOD0lw4uKt"
   },
   "source": [
    "## char-RNN in PyTorch\n",
    "\n",
    "Тут нам предстоит построить character-level RNN c помощью PyTroch. Для того чтобы освежить свои знания о данной модели рекомендую к прочтению [сию статью](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) написанную Андреем Карпатым.  Сразу проясню, тут мы будем "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "soNaU_x24uKt"
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import io\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AuC_LhMY4uKv"
   },
   "source": [
    "Обновите текст, если что-то потерлось. Можно сделать это не только в памяти компьютера, но и в своей собственной."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RkuXhwlj4uKw"
   },
   "outputs": [],
   "source": [
    "with io.open('dostoevsky.txt', 'r',encoding='utf8') as f:\n",
    "    text = f.read().replace(u'\\xa0', u' ').replace(u'\\ufeff','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JQ03nFMv4uKy"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Федор Михайлович Достоевский\\nБедные люди\\nОх уж эти мне сказочники! Нет чтобы написать что-нибудь полезное, приятное, усладительное, а то всю подноготную в земле вырывают!.. Вот уж запретил бы им писать! Ну, на что это похоже: читаешь… невольно задумываешься, — а там всякая дребедень и пойдет в голов'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pK4m1Sl54uK3"
   },
   "source": [
    "Закодируем наш текст в цифры, как мы и обсуждали ранее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9W7N9Vwv4uK4"
   },
   "outputs": [],
   "source": [
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ua3-2ZuN4uK7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11321980,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded.shape # Наш словарь получился очень большой"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IsNhlzn34uK8"
   },
   "source": [
    "#### Обработка данных\n",
    "\n",
    "Будем использовать для представления букв one-hot вектора. Напишите функцию для этого."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jf5GQulo4uK-"
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # Инициализируем вектора \n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    \n",
    "    # заполним 1 в соответсвующем месте\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Приводим к нужному размеру\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SLmKXB494uLA"
   },
   "source": [
    "Попробуйте сделать функцию ниже, которая строит генератор мини-батчей. Каждая последовательность будет длины `n_steps`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XWR0XuI14uLA"
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, n_seqs, n_steps):\n",
    "    \"\"\"\n",
    "    Создание генератора, возвращающего минибатчи размера (n_seqs x seq_len) Numpy\n",
    "    \"\"\"    \n",
    "    batch_size = n_seqs * n_steps\n",
    "    n_batches = len(arr)//batch_size\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size]\n",
    "    # Reshape into n_seqs rows\n",
    "    arr = arr.reshape((n_seqs, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        # The features\n",
    "        x = arr[:, n:n+n_steps]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+n_steps]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zc1bkSne4uLC"
   },
   "source": [
    "### Построение charRNN модели\n",
    "\n",
    "Ниже вам будет необходимо написать свою char-rnn по данному описанию. Как всегда основные рекомендации: сначала пишем определение слоев в  init, затем описываем их вызов в forward.\n",
    "\n",
    "\n",
    "Кроме того тут есть важная функция predict. (в блоге Карпатого очень подробно все описано и дабы не заниматься копипастой, я направляю вас туда). Для начала попробуйте проверить учится ли ваша сеть и только потом заполняйте метод predict.\n",
    "\n",
    "Почему даем именно модель над символами? Потому что это очень простая структура, которая может быть применена не только к текстам, но и к музыке. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "38Qqu4TbMBit"
   },
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, tokens, n_steps=100, n_hidden=256, n_layers=2,\n",
    "                               drop_prob=0.1, lr=0.001, use_embeddings=False):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        self.use_embeddings = use_embeddings\n",
    "        \n",
    "        self.chars = tokens\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        self.dropout = nn.Dropout(self.drop_prob)\n",
    "        if self.use_embeddings:\n",
    "            emb_size = 64\n",
    "            self.embedder = nn.Embedding(self.vocab_size, emb_size)\n",
    "            self.lstm = nn.GRU(emb_size, self.n_hidden, self.n_layers,\n",
    "                               batch_first=True, dropout=self.drop_prob)\n",
    "        else:\n",
    "            self.lstm = nn.GRU(self.vocab_size, self.n_hidden, self.n_layers,\n",
    "                               batch_first=True, dropout=self.drop_prob)\n",
    "        self.fc = nn.Linear(n_hidden, self.vocab_size)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self, x, hc):\n",
    "        ''' Forward pass through the network '''\n",
    "        if self.use_embeddings:\n",
    "            x = self.embedder(torch.argmax(x, dim=-1).long())\n",
    "        x, h = self.lstm(x, hc)\n",
    "#         x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x, h\n",
    "    \n",
    "    def predict(self, char, h=None, top_k=None):\n",
    "        \"\"\"        \n",
    "            Returns the predicted character and the hidden state.\n",
    "        \"\"\"\n",
    "        \n",
    "        if h is None:\n",
    "            h = self.init_hidden(1).to(device)\n",
    "        \n",
    "        x = np.array([[self.char2int[char]]])\n",
    "        x = one_hot_encode(x, self.vocab_size)\n",
    "        \n",
    "        inputs = torch.from_numpy(x).to(device)\n",
    "        out, h = self.forward(inputs, h)\n",
    "\n",
    "        p = F.softmax(out, dim=-1)\n",
    "        \n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(self.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.cpu().numpy().squeeze()\n",
    "        \n",
    "        p = p.cpu().detach().numpy().squeeze()\n",
    "        # Choose 1/k \n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        \n",
    "        return self.int2char[char], h\n",
    "    \n",
    "    def init_weights(self):\n",
    "        ''' Initialize weights for fully connected layer '''        \n",
    "        # Set bias tensor to all zeros\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        # FC weights as random uniform\n",
    "        self.fc.weight.data.uniform_(-1, 1)\n",
    "        \n",
    "    def init_hidden(self, n_seqs):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x n_seqs x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        return weight.new(self.n_layers, n_seqs, self.n_hidden).zero_()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "46Q9uU754uLE"
   },
   "source": [
    "Для проверки функционирования у нас есть функция `train` , которая позволит провести вам большое число экспериментов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C99mWJAT4uLE"
   },
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, n_seqs=10, n_steps=50, lr=0.0005, clip=3, val_frac=0.1, cuda=False, print_every=200):\n",
    "    net.train()\n",
    "#     opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        h = net.init_hidden(n_seqs)\n",
    "        for x, y in get_batches(data, n_seqs, n_steps):\n",
    "            counter += 1\n",
    "            \n",
    "            # Кодируем данные и отправлячем\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs = torch.from_numpy(x).to(device)\n",
    "            targets = torch.from_numpy(y).to(device)\n",
    "            \n",
    "            #  замените на .copy будет работать стабильнее\n",
    "            h = torch.tensor(h.data, device=device)\n",
    "\n",
    "            net.zero_grad()\n",
    "            \n",
    "            output, h = net.forward(inputs, h)\n",
    "            \n",
    "            loss = criterion(output.view(n_seqs*n_steps,-1), targets.view(n_seqs*n_steps))\n",
    "            loss.backward()\n",
    "            \n",
    "            # clip grad norm может вам помочь\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), 1)\n",
    "\n",
    "            opt.step()\n",
    "            \n",
    "            if counter % print_every == 0:\n",
    "                net.eval()\n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(n_seqs)\n",
    "                val_losses = []\n",
    "                for x, y in get_batches(val_data, n_seqs, n_steps):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    inputs = torch.from_numpy(x).to(device)\n",
    "                    targets = torch.from_numpy(y).to(device)\n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = val_h.clone().detach()\n",
    "                    output, val_h = net.forward(inputs, val_h)\n",
    "                    val_loss = criterion(output.view(n_seqs*n_steps,-1), targets.view(n_seqs*n_steps))\n",
    "                    val_losses.append(val_loss.item())\n",
    "                   \n",
    "                # Попробуем валидироваться таким способом\n",
    "                prime = 'Дом '\n",
    "                top_k = 2\n",
    "                chars = [ch for ch in prime]\n",
    "                vh = None\n",
    "                for ch in prime:\n",
    "                    char, vh = net.predict(ch, vh, top_k=top_k)\n",
    "                for ii in range(10):\n",
    "                    char, vh = net.predict(chars[-1], vh, top_k=top_k)\n",
    "                    chars.append(char)\n",
    "                    \n",
    "                #chars.append(char)\n",
    "                print(''.join(chars))\n",
    "                net.train()\n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)),\n",
    "                       \"Validation Perplexity: {:.4f}\".format(np.exp(np.mean(val_losses))))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pvSblHi44uLG"
   },
   "source": [
    "## Время тренировки!\n",
    "\n",
    " \n",
    "Теперь мы можем тренировать сеть. Сначала мы создадим саму сеть, с некоторыми заданными гиперпараметрами. Затем определите размеры мини-партий (количество последовательностей и количество шагов) и начните обучение. С функцией поезда мы можем установить количество эпох, скорость обучения и другие параметры. Кроме того, мы можем запустить обучение на графическом процессоре, установив `cuda = True`. Сейчас google дает всем бесплатно использовать gpu с помощью сервиса codelab.\n",
    "\n",
    "\n",
    "![a](https://www.apmpodcasts.org/wp-content/uploads/2015/06/adventure-time.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hAcQeadQ4uLH"
   },
   "outputs": [],
   "source": [
    "if 'net' in locals():\n",
    "    del net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qBBOtwJM4uLJ"
   },
   "outputs": [],
   "source": [
    "num_hidden_units = 512\n",
    "use_embeddings = True\n",
    "net = CharRNN(chars, n_hidden=num_hidden_units, n_layers=2, use_embeddings=use_embeddings).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gjSO9VtL4uLM",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mikhail/.local/lib/python3.6/site-packages/ipykernel_launcher.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Дом всем него \n",
      "Epoch: 1/15... Step: 100... Loss: 2.2304... Val Loss: 2.1759 Validation Perplexity: 8.8098\n",
      "Дом возможно б\n",
      "Epoch: 1/15... Step: 200... Loss: 2.0138... Val Loss: 1.9891 Validation Perplexity: 7.3090\n",
      "Дом никак он п\n",
      "Epoch: 1/15... Step: 300... Loss: 1.8952... Val Loss: 1.8774 Validation Perplexity: 6.5366\n",
      "Дом воспританн\n",
      "Epoch: 1/15... Step: 400... Loss: 1.8167... Val Loss: 1.7967 Validation Perplexity: 6.0296\n",
      "Дом возможно в\n",
      "Epoch: 1/15... Step: 500... Loss: 1.7758... Val Loss: 1.7424 Validation Perplexity: 5.7108\n",
      "Дом войдет, и \n",
      "Epoch: 1/15... Step: 600... Loss: 1.6853... Val Loss: 1.6869 Validation Perplexity: 5.4028\n",
      "Дом полинение \n",
      "Epoch: 1/15... Step: 700... Loss: 1.6836... Val Loss: 1.6476 Validation Perplexity: 5.1947\n",
      "Дом ней наше в\n",
      "Epoch: 2/15... Step: 800... Loss: 1.6458... Val Loss: 1.6203 Validation Perplexity: 5.0547\n",
      "Дом них на пол\n",
      "Epoch: 2/15... Step: 900... Loss: 1.6180... Val Loss: 1.5917 Validation Perplexity: 4.9121\n",
      "Дом полным про\n",
      "Epoch: 2/15... Step: 1000... Loss: 1.5481... Val Loss: 1.5708 Validation Perplexity: 4.8107\n",
      "Дом полном, ка\n",
      "Epoch: 2/15... Step: 1100... Loss: 1.5336... Val Loss: 1.5504 Validation Perplexity: 4.7133\n",
      "Дом прежнему с\n",
      "Epoch: 2/15... Step: 1200... Loss: 1.5643... Val Loss: 1.5290 Validation Perplexity: 4.6138\n",
      "Дом нем положи\n",
      "Epoch: 2/15... Step: 1300... Loss: 1.5224... Val Loss: 1.5161 Validation Perplexity: 4.5547\n",
      "Дом получении \n",
      "Epoch: 2/15... Step: 1400... Loss: 1.4975... Val Loss: 1.5033 Validation Perplexity: 4.4965\n",
      "Дом постоянном\n",
      "Epoch: 2/15... Step: 1500... Loss: 1.4689... Val Loss: 1.4872 Validation Perplexity: 4.4247\n",
      "Дом полу не по\n",
      "Epoch: 3/15... Step: 1600... Loss: 1.5192... Val Loss: 1.4752 Validation Perplexity: 4.3719\n",
      "Дом последней \n",
      "Epoch: 3/15... Step: 1700... Loss: 1.4468... Val Loss: 1.4674 Validation Perplexity: 4.3380\n",
      "Дом него от не\n",
      "Epoch: 3/15... Step: 1800... Loss: 1.4511... Val Loss: 1.4596 Validation Perplexity: 4.3044\n",
      "Дом положитель\n",
      "Epoch: 3/15... Step: 1900... Loss: 1.4494... Val Loss: 1.4554 Validation Perplexity: 4.2861\n",
      "Дом прежних пр\n",
      "Epoch: 3/15... Step: 2000... Loss: 1.4085... Val Loss: 1.4406 Validation Perplexity: 4.2233\n",
      "Дом нем ничего\n",
      "Epoch: 3/15... Step: 2100... Loss: 1.4256... Val Loss: 1.4323 Validation Perplexity: 4.1882\n",
      "Дом нем начина\n",
      "Epoch: 3/15... Step: 2200... Loss: 1.4033... Val Loss: 1.4266 Validation Perplexity: 4.1645\n",
      "Дом них на все\n",
      "Epoch: 3/15... Step: 2300... Loss: 1.3874... Val Loss: 1.4175 Validation Perplexity: 4.1269\n",
      "Дом нем никогд\n",
      "Epoch: 4/15... Step: 2400... Loss: 1.3922... Val Loss: 1.4132 Validation Perplexity: 4.1090\n",
      "Дом последних \n",
      "Epoch: 4/15... Step: 2500... Loss: 1.3849... Val Loss: 1.4086 Validation Perplexity: 4.0900\n",
      "Дом него не ск\n",
      "Epoch: 4/15... Step: 2600... Loss: 1.3897... Val Loss: 1.4048 Validation Perplexity: 4.0749\n",
      "Дом не понимаю\n",
      "Epoch: 4/15... Step: 2700... Loss: 1.4056... Val Loss: 1.4000 Validation Perplexity: 4.0553\n",
      "Дом не подумыв\n",
      "Epoch: 4/15... Step: 2800... Loss: 1.3557... Val Loss: 1.3916 Validation Perplexity: 4.0213\n",
      "Дом покровище \n",
      "Epoch: 4/15... Step: 2900... Loss: 1.3415... Val Loss: 1.3891 Validation Perplexity: 4.0114\n",
      "Дом прием пока\n",
      "Epoch: 4/15... Step: 3000... Loss: 1.3554... Val Loss: 1.3853 Validation Perplexity: 3.9961\n",
      "Дом несколько \n",
      "Epoch: 4/15... Step: 3100... Loss: 1.3357... Val Loss: 1.3817 Validation Perplexity: 3.9818\n",
      "Дом под конце \n",
      "Epoch: 5/15... Step: 3200... Loss: 1.3337... Val Loss: 1.3754 Validation Perplexity: 3.9567\n",
      "Дом них не пон\n",
      "Epoch: 5/15... Step: 3300... Loss: 1.2935... Val Loss: 1.3683 Validation Perplexity: 3.9287\n",
      "Дом прокламаци\n",
      "Epoch: 5/15... Step: 3400... Loss: 1.3569... Val Loss: 1.3703 Validation Perplexity: 3.9365\n",
      "Дом известных \n",
      "Epoch: 5/15... Step: 3500... Loss: 1.3456... Val Loss: 1.3680 Validation Perplexity: 3.9275\n",
      "Дом из нас в М\n",
      "Epoch: 5/15... Step: 3600... Loss: 1.3398... Val Loss: 1.3642 Validation Perplexity: 3.9124\n",
      "Дом просидевше\n",
      "Epoch: 5/15... Step: 3700... Loss: 1.2906... Val Loss: 1.3564 Validation Perplexity: 3.8823\n",
      "Дом их в садом\n",
      "Epoch: 5/15... Step: 3800... Loss: 1.3170... Val Loss: 1.3574 Validation Perplexity: 3.8862\n",
      "Дом не будет д\n",
      "Epoch: 5/15... Step: 3900... Loss: 1.3031... Val Loss: 1.3524 Validation Perplexity: 3.8665\n",
      "Дом них и не о\n",
      "Epoch: 6/15... Step: 4000... Loss: 1.2987... Val Loss: 1.3519 Validation Perplexity: 3.8646\n",
      "Дом последней \n",
      "Epoch: 6/15... Step: 4100... Loss: 1.2824... Val Loss: 1.3464 Validation Perplexity: 3.8437\n",
      "Дом несколько \n",
      "Epoch: 6/15... Step: 4200... Loss: 1.2987... Val Loss: 1.3473 Validation Perplexity: 3.8471\n",
      "Дом него в сво\n",
      "Epoch: 6/15... Step: 4300... Loss: 1.2881... Val Loss: 1.3502 Validation Perplexity: 3.8584\n",
      "Дом не может п\n",
      "Epoch: 6/15... Step: 4400... Loss: 1.2877... Val Loss: 1.3418 Validation Perplexity: 3.8260\n",
      "Дом не будет н\n",
      "Epoch: 6/15... Step: 4500... Loss: 1.2827... Val Loss: 1.3382 Validation Perplexity: 3.8124\n",
      "Дом не было ни\n",
      "Epoch: 6/15... Step: 4600... Loss: 1.2808... Val Loss: 1.3381 Validation Perplexity: 3.8119\n",
      "Дом не было ни\n",
      "Epoch: 6/15... Step: 4700... Loss: 1.2923... Val Loss: 1.3349 Validation Perplexity: 3.7996\n",
      "Дом не было не\n",
      "Epoch: 7/15... Step: 4800... Loss: 1.2635... Val Loss: 1.3336 Validation Perplexity: 3.7948\n",
      "Дом под руку, \n",
      "Epoch: 7/15... Step: 4900... Loss: 1.2561... Val Loss: 1.3324 Validation Perplexity: 3.7903\n",
      "Дом несчастным\n",
      "Epoch: 7/15... Step: 5000... Loss: 1.3071... Val Loss: 1.3283 Validation Perplexity: 3.7745\n",
      "Дом исполнител\n",
      "Epoch: 7/15... Step: 5100... Loss: 1.2861... Val Loss: 1.3290 Validation Perplexity: 3.7774\n",
      "Дом нем не про\n",
      "Epoch: 7/15... Step: 5200... Loss: 1.2541... Val Loss: 1.3295 Validation Perplexity: 3.7791\n",
      "Дом поступок в\n",
      "Epoch: 7/15... Step: 5300... Loss: 1.2616... Val Loss: 1.3273 Validation Perplexity: 3.7708\n",
      "Дом недоумение\n",
      "Epoch: 7/15... Step: 5400... Loss: 1.2408... Val Loss: 1.3232 Validation Perplexity: 3.7555\n",
      "Дом не можем д\n",
      "Epoch: 7/15... Step: 5500... Loss: 1.2662... Val Loss: 1.3210 Validation Perplexity: 3.7470\n",
      "Дом не может б\n",
      "Epoch: 8/15... Step: 5600... Loss: 1.2436... Val Loss: 1.3226 Validation Perplexity: 3.7531\n",
      "Дом насмешку, \n",
      "Epoch: 8/15... Step: 5700... Loss: 1.2515... Val Loss: 1.3210 Validation Perplexity: 3.7471\n",
      "Дом несчастног\n",
      "Epoch: 8/15... Step: 5800... Loss: 1.2572... Val Loss: 1.3162 Validation Perplexity: 3.7293\n",
      "Дом словах сво\n",
      "Epoch: 8/15... Step: 5900... Loss: 1.2583... Val Loss: 1.3175 Validation Perplexity: 3.7342\n",
      "Дом несчастных\n",
      "Epoch: 8/15... Step: 6000... Loss: 1.2825... Val Loss: 1.3149 Validation Perplexity: 3.7243\n",
      "Дом не может п\n",
      "Epoch: 8/15... Step: 6100... Loss: 1.2517... Val Loss: 1.3150 Validation Perplexity: 3.7248\n",
      "Дом известие о\n",
      "Epoch: 8/15... Step: 6200... Loss: 1.2231... Val Loss: 1.3126 Validation Perplexity: 3.7158\n",
      "Дом случай сво\n",
      "Epoch: 8/15... Step: 6300... Loss: 1.2861... Val Loss: 1.3117 Validation Perplexity: 3.7126\n",
      "Дом после свое\n",
      "Epoch: 9/15... Step: 6400... Loss: 1.2144... Val Loss: 1.3124 Validation Perplexity: 3.7150\n",
      "Дом несчастный\n",
      "Epoch: 9/15... Step: 6500... Loss: 1.2290... Val Loss: 1.3151 Validation Perplexity: 3.7251\n",
      "Дом немецкому,\n",
      "Epoch: 9/15... Step: 6600... Loss: 1.2475... Val Loss: 1.3090 Validation Perplexity: 3.7024\n",
      "Дом служанки с\n",
      "Epoch: 9/15... Step: 6700... Loss: 1.2670... Val Loss: 1.3099 Validation Perplexity: 3.7057\n",
      "Дом ничего не \n",
      "Epoch: 9/15... Step: 6800... Loss: 1.2597... Val Loss: 1.3100 Validation Perplexity: 3.7060\n",
      "Дом немецких п\n",
      "Epoch: 9/15... Step: 6900... Loss: 1.2376... Val Loss: 1.3078 Validation Perplexity: 3.6982\n",
      "Дом и с толку \n",
      "Epoch: 9/15... Step: 7000... Loss: 1.2592... Val Loss: 1.3060 Validation Perplexity: 3.6913\n",
      "Дом сердцем св\n",
      "Epoch: 9/15... Step: 7100... Loss: 1.2351... Val Loss: 1.3046 Validation Perplexity: 3.6861\n",
      "Дом не будущее\n",
      "Epoch: 10/15... Step: 7200... Loss: 1.2371... Val Loss: 1.3044 Validation Perplexity: 3.6856\n",
      "Дом сердцем в \n",
      "Epoch: 10/15... Step: 7300... Loss: 1.2341... Val Loss: 1.3037 Validation Perplexity: 3.6831\n",
      "Дом начинается\n",
      "Epoch: 10/15... Step: 7400... Loss: 1.2285... Val Loss: 1.3009 Validation Perplexity: 3.6726\n",
      "Дом известных \n",
      "Epoch: 10/15... Step: 7500... Loss: 1.2320... Val Loss: 1.3034 Validation Perplexity: 3.6818\n",
      "Дом него светл\n",
      "Epoch: 10/15... Step: 7600... Loss: 1.2273... Val Loss: 1.3015 Validation Perplexity: 3.6749\n",
      "Дом из него не\n",
      "Epoch: 10/15... Step: 7700... Loss: 1.2153... Val Loss: 1.3004 Validation Perplexity: 3.6708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Дом сердцем бо\n",
      "Epoch: 10/15... Step: 7800... Loss: 1.2722... Val Loss: 1.3012 Validation Perplexity: 3.6736\n",
      "Дом известиями\n",
      "Epoch: 10/15... Step: 7900... Loss: 1.1993... Val Loss: 1.2993 Validation Perplexity: 3.6667\n",
      "Дом отца нашег\n",
      "Epoch: 11/15... Step: 8000... Loss: 1.2194... Val Loss: 1.2996 Validation Perplexity: 3.6680\n",
      "Дом последний \n",
      "Epoch: 11/15... Step: 8100... Loss: 1.2526... Val Loss: 1.2961 Validation Perplexity: 3.6551\n",
      "Дом себе полож\n",
      "Epoch: 11/15... Step: 8200... Loss: 1.2052... Val Loss: 1.2988 Validation Perplexity: 3.6648\n",
      "Дом начинают с\n",
      "Epoch: 11/15... Step: 8300... Loss: 1.2067... Val Loss: 1.2965 Validation Perplexity: 3.6564\n",
      "Дом из них не \n",
      "Epoch: 11/15... Step: 8400... Loss: 1.2338... Val Loss: 1.2965 Validation Perplexity: 3.6566\n",
      "Дом немец подо\n",
      "Epoch: 11/15... Step: 8500... Loss: 1.2293... Val Loss: 1.2964 Validation Perplexity: 3.6561\n",
      "Дом их принять\n",
      "Epoch: 11/15... Step: 8600... Loss: 1.2207... Val Loss: 1.2921 Validation Perplexity: 3.6404\n",
      "Дом и почти не\n",
      "Epoch: 11/15... Step: 8700... Loss: 1.2163... Val Loss: 1.2929 Validation Perplexity: 3.6435\n",
      "Дом их домой, \n",
      "Epoch: 12/15... Step: 8800... Loss: 1.1793... Val Loss: 1.2962 Validation Perplexity: 3.6552\n",
      "Дом страдалице\n",
      "Epoch: 12/15... Step: 8900... Loss: 1.2222... Val Loss: 1.2954 Validation Perplexity: 3.6526\n",
      "Дом несколько \n",
      "Epoch: 12/15... Step: 9000... Loss: 1.2115... Val Loss: 1.2896 Validation Perplexity: 3.6315\n",
      "Дом известной \n",
      "Epoch: 12/15... Step: 9100... Loss: 1.1934... Val Loss: 1.2919 Validation Perplexity: 3.6397\n",
      "Дом из комнаты\n",
      "Epoch: 12/15... Step: 9200... Loss: 1.2103... Val Loss: 1.2918 Validation Perplexity: 3.6395\n",
      "Дом их был нес\n",
      "Epoch: 12/15... Step: 9300... Loss: 1.1902... Val Loss: 1.2932 Validation Perplexity: 3.6444\n",
      "Дом столько вр\n",
      "Epoch: 12/15... Step: 9400... Loss: 1.1700... Val Loss: 1.2936 Validation Perplexity: 3.6459\n",
      "Дом из возвыше\n",
      "Epoch: 12/15... Step: 9500... Loss: 1.1988... Val Loss: 1.2895 Validation Perplexity: 3.6310\n",
      "Дом слова о пр\n",
      "Epoch: 13/15... Step: 9600... Loss: 1.1637... Val Loss: 1.2913 Validation Perplexity: 3.6375\n",
      "Дом сердце ее \n",
      "Epoch: 13/15... Step: 9700... Loss: 1.2075... Val Loss: 1.2872 Validation Perplexity: 3.6225\n",
      "Дом известных \n",
      "Epoch: 13/15... Step: 9800... Loss: 1.2137... Val Loss: 1.2876 Validation Perplexity: 3.6241\n",
      "Дом из себе от\n",
      "Epoch: 13/15... Step: 9900... Loss: 1.1764... Val Loss: 1.2881 Validation Perplexity: 3.6258\n",
      "Дом из них не \n",
      "Epoch: 13/15... Step: 10000... Loss: 1.1899... Val Loss: 1.2891 Validation Perplexity: 3.6294\n",
      "Дом словам не \n",
      "Epoch: 13/15... Step: 10100... Loss: 1.1765... Val Loss: 1.2880 Validation Perplexity: 3.6256\n",
      "Дом из нашего \n",
      "Epoch: 13/15... Step: 10200... Loss: 1.1975... Val Loss: 1.2880 Validation Perplexity: 3.6256\n",
      "Дом из наших, \n",
      "Epoch: 13/15... Step: 10300... Loss: 1.1545... Val Loss: 1.2895 Validation Perplexity: 3.6308\n",
      "Дом из них, и \n",
      "Epoch: 14/15... Step: 10400... Loss: 1.1590... Val Loss: 1.2857 Validation Perplexity: 3.6172\n",
      "Дом изверги до\n",
      "Epoch: 14/15... Step: 10500... Loss: 1.2211... Val Loss: 1.2872 Validation Perplexity: 3.6227\n",
      "Дом и вознесен\n",
      "Epoch: 14/15... Step: 10600... Loss: 1.2049... Val Loss: 1.2853 Validation Perplexity: 3.6156\n",
      "Дом из нас не \n",
      "Epoch: 14/15... Step: 10700... Loss: 1.1624... Val Loss: 1.2829 Validation Perplexity: 3.6069\n",
      "Дом их возвест\n",
      "Epoch: 14/15... Step: 10800... Loss: 1.1735... Val Loss: 1.2888 Validation Perplexity: 3.6283\n",
      "Дом случается \n",
      "Epoch: 14/15... Step: 10900... Loss: 1.1863... Val Loss: 1.2861 Validation Perplexity: 3.6187\n",
      "Дом известной \n",
      "Epoch: 14/15... Step: 11000... Loss: 1.1806... Val Loss: 1.2891 Validation Perplexity: 3.6294\n",
      "Дом несчастный\n",
      "Epoch: 14/15... Step: 11100... Loss: 1.1694... Val Loss: 1.2863 Validation Perplexity: 3.6195\n",
      "Дом остальные \n",
      "Epoch: 15/15... Step: 11200... Loss: 1.1633... Val Loss: 1.2856 Validation Perplexity: 3.6170\n",
      "Дом своей стор\n",
      "Epoch: 15/15... Step: 11300... Loss: 1.1791... Val Loss: 1.2839 Validation Perplexity: 3.6107\n",
      "Дом случае сов\n",
      "Epoch: 15/15... Step: 11400... Loss: 1.2087... Val Loss: 1.2882 Validation Perplexity: 3.6264\n",
      "Дом из них под\n",
      "Epoch: 15/15... Step: 11500... Loss: 1.1652... Val Loss: 1.2858 Validation Perplexity: 3.6176\n",
      "Дом их, которо\n",
      "Epoch: 15/15... Step: 11600... Loss: 1.1332... Val Loss: 1.2855 Validation Perplexity: 3.6165\n",
      "Дом известие о\n",
      "Epoch: 15/15... Step: 11700... Loss: 1.1749... Val Loss: 1.2833 Validation Perplexity: 3.6087\n",
      "Дом их в комна\n",
      "Epoch: 15/15... Step: 11800... Loss: 1.1662... Val Loss: 1.2824 Validation Perplexity: 3.6054\n",
      "Дом известных \n",
      "Epoch: 15/15... Step: 11900... Loss: 1.1677... Val Loss: 1.2871 Validation Perplexity: 3.6221\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CharRNN(\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (embedder): Embedding(175, 64)\n",
       "  (lstm): GRU(64, 512, num_layers=2, batch_first=True, dropout=0.1)\n",
       "  (fc): Linear(in_features=512, out_features=175, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_seqs, n_steps = 128, 100\n",
    "print_every = 100\n",
    "train(net, encoded, epochs=15, n_seqs=n_seqs, n_steps=n_steps, lr=0.0005, cuda=True, print_every=print_every)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ff7gRb4F4uLO"
   },
   "source": [
    "![train](https://i.pinimg.com/474x/0e/58/69/0e5869297852211e8447d6b09fa1f4f5.jpg)\n",
    "\n",
    "**Вопрос 6**\n",
    "\n",
    "Введите целую часть итоговой  перплексии ex. [142.37 ] = 142\n",
    "\n",
    "3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TGHV-y2d4uLQ"
   },
   "source": [
    "### Загрузка модели \n",
    "\n",
    "Чтобы настроить гиперпараметры для получения максимальной производительности, вам понадобятся наблюдения за обучением и валидацией. Если ваша потеря обучения намного ниже, чем потеря проверки, вы перерабатываете. Увеличьте регуляризацию (больше выпадений) или используйте меньшую сеть. Если потери обучения и проверки близки, вы недофинансируете, чтобы увеличить размер сети."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "76hVSahj4uLQ"
   },
   "source": [
    "После обучения мы сохраним модель, чтобы мы могли загрузить ее позже, если понадобится. Здесь сохраняются параметры, необходимые для создания той же архитектуры, гиперпараметров скрытого слоя и текстовых символов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LzWyveW24uLR"
   },
   "outputs": [],
   "source": [
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars,\n",
    "               'use_embeddings': use_embeddings}\n",
    "with open('./rnn.net', 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kmMeiwj_4uLU"
   },
   "source": [
    "## Семплирование\n",
    "\n",
    "Теперь, когда модель обучена, мы захотим попробовать ее. Чтобы заполучить текст, мы передаем символ и сеть прогнозируем следующий символ. Затем мы берем новый символ, передаем его обратно и получаем еще один. Просто продолжайте делать это, и вы создадите кучу текста!\n",
    "\n",
    "### Top K \n",
    "\n",
    "Наши прогнозы основаны на категориальном распределении вероятностей по всем возможным признакам. Мы можем сделать выборку более разумным, но менее переменным, учитывая только некоторые вероятные символы $ K $. Это будет препятствовать тому, чтобы сеть давала нам абсолютно абсурдные символы, позволяя ей вводить некоторый шум и случайность в выбранный текст.\n",
    "\n",
    "Как правило, вы хотите настроить сеть, чтобы создать скрытое состояние. В противном случае сеть начнет генерировать символы наугад.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F83uekj74uLU"
   },
   "source": [
    "Из основного задания, попробуйте сгенерировать небольшой рассказик. Поиграйте с параметрами. Найдите собственный датасет( чтобы проверяющим было весело ) и попробуйте обучить нейронную сеть на нем. \n",
    "\n",
    "Не забывайте эксперементировать со структурой. У LSTM много аналогий, посмотрите как они справляются с данной задачей и запишите это в отчет. \n",
    "\n",
    "**Вопрос 7**\n",
    "\n",
    "Введите результат модели. Помните, он должен быть осмысленным.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9nLlCmPO4uLV"
   },
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None, cuda=False):\n",
    "    net.eval()\n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = None\n",
    "    for ch in prime:\n",
    "        char, h = net.predict(ch, h,top_k=top_k)\n",
    "        \n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = net.predict(chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u6toyTr04uLX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Путина, и все в ней возросло и в том, что он все веруют, что в таком случае \n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "print(sample(net, 70, prime='Путин', top_k=2, cuda=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2ISqy7dUYFyf"
   },
   "source": [
    "Посмотрим на семлы случайной модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CE_omhdlFnjs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Путин JИУJZJJИ:Z:ОJZИ:JИJИО\n"
     ]
    }
   ],
   "source": [
    "print(sample(CharRNN(chars, n_hidden=num_hidden_units, n_layers=2).to(device), \n",
    "             20, prime='Путин ', top_k=5, cuda=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3ep25-RYYJEW"
   },
   "source": [
    "Разница видна, значит мы все сделали правильно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r1XJ-2PT4uLY"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./rnn.net', 'rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "    \n",
    "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'], use_embeddings=checkpoint['use_embeddings'])\n",
    "loaded.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BZisXBVB4uLa"
   },
   "source": [
    "## Дополнительная часть\n",
    "\n",
    "\n",
    "Тут предлагается описать все проделанные эксперименты. \n",
    "\n",
    "Попробуйте сделать его насыщенным необходимой информацией для проверяющих, чтобы можно было сразу оценить сколько было проделано работы. Пример вопросов для отчета был дан выше. Можно добавить семплы модели на эпохах 1, 3, 9, 15, для лучшего понимания обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| hidden units   |use embeddings   | validation perplexity   |  \n",
    "|---------|---|---|\n",
    "| 32  | False   |6.6331 |\n",
    "| 32  | True   |6.4652   |\n",
    "| 64  | False   |5.3862 |\n",
    "| 64  | True   | 5.1893  |\n",
    "| 128  | False    | 4.5528 |\n",
    "| 128  | True   | 4.4040 |\n",
    "| 512  | True   | 3.6221|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Семплы модели на эпохах 1, 3, 9, 15\n",
    "\n",
    "Epoch: 1\n",
    "    \n",
    "Дом полинение\n",
    "\n",
    "Epoch: 3\n",
    "    \n",
    "Дом них на все\n",
    "\n",
    "Epoch: 9\n",
    "    \n",
    "Дом сердцем св\n",
    "\n",
    "Epoch: 15\n",
    "    \n",
    "Дом известных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В целом увеличение как количества ячеек так и добавление слоя эмбедингов позволяло снижать перплексию. \n",
    "При этом видно, что далекие зависимости сеть не улавливает и генерирует согласованные но лишенные смысла тексты"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "[homework]LanguageModel.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
